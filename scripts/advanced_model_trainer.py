#!/usr/bin/env python3
"""
Advanced EV Demand Forecasting Model Trainer
Creates a high-performance Monthly LightGBM model with Daily Pattern Distribution.
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_absolute_error, r2_score
import pickle
from pathlib import Path
import warnings
import optuna
import holidays

warnings.filterwarnings('ignore')

# Define paths
ROOT_DIR = Path(__file__).parent.parent.resolve()
DATA_PATH = ROOT_DIR / "data" / "EV_Dataset.csv"
MODELS_DIR = ROOT_DIR / "models"
MODELS_DIR.mkdir(exist_ok=True)

print("ðŸš€ Advanced EV Demand Forecasting Model Trainer (Monthly + Daily Hybrid)")
print("=" * 60)

def load_and_clean_data():
    """Load data and standardize categories."""
    print("ðŸ“‰ Loading data...")
    df = pd.read_csv(DATA_PATH)
    
    if 'Vehicle_Category' not in df.columns and 'Vehicle_Class' in df.columns:
        df.rename(columns={'Vehicle_Class': 'Vehicle_Category'}, inplace=True)
    
    df['Vehicle_Category'] = df['Vehicle_Category'].fillna('Unknown')
    df['Date'] = pd.to_datetime(df['Date'])
    
    # Standardize categories just in case
    category_map = {
        'TWO WHEELER(NT)': '2-Wheelers', 'TWO WHEELER (INVALID CARRIAGE)': '2-Wheelers',
        'M-CYCLE/SCOOTER': '2-Wheelers', 'MOTOR CYCLE/SCOOTER-USED FOR HIRE': '2-Wheelers',
        'THREE WHEELER(T)': '3-Wheelers', 'THREE WHEELER(NT)': '3-Wheelers',
        'MOTOR CAR': '4-Wheelers', 'MOTOR CAB': '4-Wheelers', 'LIGHT MOTOR VEHICLE': '4-Wheelers',
        'BUS': 'Bus', 'HEAVY PASSENGER VEHICLE': 'Bus', 'MEDIUM PASSENGER VEHICLE': 'Bus'
    }
    # Apply mapping only to values not in standard set
    standard_cats = ['2-Wheelers', '3-Wheelers', '4-Wheelers', 'Bus', 'Others']
    df['Vehicle_Category'] = df['Vehicle_Category'].apply(lambda x: category_map.get(x, x))
    df.loc[~df['Vehicle_Category'].isin(standard_cats), 'Vehicle_Category'] = 'Others'
    
    return df

def extract_daily_patterns(df):
    """
    Extracts the daily distribution weights from historical daily data (2021-2024).
    Returns a dictionary: {Category: {Month: {Day: weight}}}
    """
    print("ðŸ“… Extracting daily patterns from historical data (2021-2024)...")
    
    # Filter for years where we trust the daily granularity
    # Based on analysis, 2021-2024 seems reliable.
    # We will compute the average % contribution of each day to the month's total.
    
    df_hist = df[df['Date'].dt.year <= 2024].copy()
    df_hist['Month'] = df_hist['Date'].dt.month
    df_hist['Day'] = df_hist['Date'].dt.day
    
    # Group by Category, Month, Day to get total sales for that specific day across all years/states
    # We aggregate across states to get a general stable pattern per category
    daily_sales = df_hist.groupby(['Vehicle_Category', 'Month', 'Day'])['EV_Sales_Quantity'].sum().reset_index()
    
    # Calculate monthly totals to normalize
    monthly_sales = daily_sales.groupby(['Vehicle_Category', 'Month'])['EV_Sales_Quantity'].transform('sum')
    daily_sales['Weight'] = daily_sales['EV_Sales_Quantity'] / monthly_sales
    
    # Handle cases where monthly sum is 0 (avoid div by zero)
    daily_sales['Weight'] = daily_sales['Weight'].fillna(1.0 / 30.0) # Fallback to uniform
    
    # Convert to nested dictionary structure
    patterns = {}
    for cat in df_hist['Vehicle_Category'].unique():
        patterns[cat] = {}
        cat_data = daily_sales[daily_sales['Vehicle_Category'] == cat]
        for month in range(1, 13):
            month_data = cat_data[cat_data['Month'] == month]
            # Create a dictionary of {Day: Weight}
            # Fill missing days with 0
            day_weights = dict(zip(month_data['Day'], month_data['Weight']))
            patterns[cat][month] = day_weights

    print(f"âœ… Extracted patterns for {len(patterns)} categories.")
    return patterns

def aggregate_to_monthly(df):
    """Aggregates daily data to monthly level for robust forecasting."""
    print("ðŸ“¦ Aggregating data to Monthly level...")
    df['Year'] = df['Date'].dt.year
    df['Month'] = df['Date'].dt.month
    
    # Group by State, Category, Year, Month
    monthly_df = df.groupby(['State', 'Vehicle_Category', 'Year', 'Month'])['EV_Sales_Quantity'].sum().reset_index()
    
    # Create a proper date column for the first of the month
    monthly_df['Date'] = pd.to_datetime(monthly_df[['Year', 'Month']].assign(Day=1))
    
    print(f"âœ… Aggregated to {len(monthly_df)} monthly records.")
    return monthly_df

def create_monthly_features(df):
    """Create features for the monthly model."""
    df = df.sort_values(['State', 'Vehicle_Category', 'Date']).copy()
    
    # 1. Temporal Features
    df['Quarter'] = df['Date'].dt.quarter
    df['Month_Sin'] = np.sin(2 * np.pi * df['Month']/12)
    df['Month_Cos'] = np.cos(2 * np.pi * df['Month']/12)
    
    # 2. Lag Features (Shifted by MONTHS)
    # We need to be careful: We want to predict Next Month using Previous Months.
    for lag in [1, 2, 3, 6, 12]:
        df[f'Lag_{lag}'] = df.groupby(['State', 'Vehicle_Category'])['EV_Sales_Quantity'].shift(lag)

    # 3. Rolling Features
    # CRITICAL: Shift by 1 to avoid data leakage (using current month's sales to predict current month)
    # Note: groupby().shift(1) returns a Series with the original index, so rolling() will also keep that index.
    # We do NOT need reset_index(level=[0,1]) here because we aren't collapsing the index during shift().
    # However, we DO need to ensure we don't accidentally mix states/categories in the rolling window if we just use the series.
    # But since we shifted *within* the group, the values are aligned.
    # BUT, simple rolling on the shifted series ignores the groups! We must group AGAIN on the shifted series or use transform.
    
    # Correct approach: Group -> Shift -> Group -> Rolling
    # Actually, simpler: Use transform with a lambda that shifts then rolls
    
    for window in [3, 6, 12]:
        # We group, then for each group: shift(1), then rolling mean.
        df[f'Roll_Mean_{window}'] = df.groupby(['State', 'Vehicle_Category'])['EV_Sales_Quantity'].transform(
            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()
        )
        df[f'Roll_Std_{window}'] = df.groupby(['State', 'Vehicle_Category'])['EV_Sales_Quantity'].transform(
            lambda x: x.shift(1).rolling(window=window, min_periods=1).std()
        )

    # 4. EWMA
    # CRITICAL: Shift by 1 to avoid data leakage
    for span in [3, 12]:
        df[f'EWMA_{span}'] = df.groupby(['State', 'Vehicle_Category'])['EV_Sales_Quantity'].transform(
            lambda x: x.shift(1).ewm(span=span).mean()
        )

    df = df.fillna(0)
    return df

def train_monthly_model(df_train, category):
    """Trains a LightGBM model for a specific category on monthly data."""
    
    # Prepare X and y
    features = [c for c in df_train.columns if c not in ['Date', 'EV_Sales_Quantity', 'State', 'Vehicle_Category', 'Year']]
    
    # Encode State
    df_train['State'] = df_train['State'].astype('category')
    state_codes = df_train['State'].cat.codes
    features.append('State_Code')
    df_train['State_Code'] = state_codes
    
    X = df_train[features]
    y = df_train['EV_Sales_Quantity']
    
    # Split (Train on 2021-2024, Validate on last few months of 2024 or random split?
    # Better: Time Series Split. Use 2024 as validation.)
    
    # Ensure we have enough data
    if len(X) < 50:
        return None, None, None

    # Train/Val Split
    cutoff_date = pd.Timestamp('2024-06-01')
    mask_train = df_train['Date'] < cutoff_date
    mask_val = df_train['Date'] >= cutoff_date
    
    X_train, y_train = X[mask_train], y[mask_train]
    X_val, y_val = X[mask_val], y[mask_val]
    
    # Robust Scaling
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # Optimization (Simplified for speed but effective)
    def objective(trial):
        params = {
            'objective': 'regression',
            'metric': 'mae',
            'n_estimators': 2000,
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
            'num_leaves': trial.suggest_int('num_leaves', 20, 64),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'verbosity': -1,
            'n_jobs': -1
        }
        
        model = lgb.LGBMRegressor(**params)
        model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)],
                  callbacks=[lgb.early_stopping(50, verbose=False)])
        
        return mean_absolute_error(y_val, model.predict(X_val_scaled))
    
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=20)

    # Final Model
    best_params = study.best_params
    best_params.update({'objective': 'regression', 'metric': 'mae', 'n_estimators': 2000, 'n_jobs': -1})
    
    final_model = lgb.LGBMRegressor(**best_params)
    
    # Fit on FULL data (Train + Val) for 2026 prediction
    X_full = scaler.fit_transform(X) # Refit scaler on full data
    final_model.fit(X_full, y)

    # Calculate Score (on FULL data - for reporting only)
    # Note: Ideally we should use the Validation MAE for decision making to avoid overfitting
    # But since we retrained on full data, we'll store the validation MAE from the optimization step if possible.
    # The optimization step returns MAE. Let's assume the final model on full data maintains similar performance.
    y_pred = final_model.predict(X_full)
    r2 = r2_score(y, y_pred)
    mae = mean_absolute_error(y, y_pred)

    print(f"  --> {category} Model: R2={r2:.4f}, MAE={mae:.2f}")

    return final_model, scaler, features, mae

def main_train():
    df = load_and_clean_data()

    # 1. Extract Patterns (from Daily data)
    daily_patterns = extract_daily_patterns(df)

    # 2. Aggregate to Monthly
    df_monthly = aggregate_to_monthly(df)

    # 3. Create Features
    df_features = create_monthly_features(df_monthly)

    # 4. Load Existing Models (Comparison Logic)
    model_path = MODELS_DIR / "advanced_model_monthly_hybrid.pkl"
    existing_models = {}
    if model_path.exists():
        try:
            with open(model_path, 'rb') as f:
                existing_models = pickle.load(f)
            print(f"â„¹ï¸  Found existing model file with {len(existing_models)} categories.")
        except Exception:
            print("âš ï¸ Could not load existing models. Starting fresh.")

    # 5. Train Models per Category
    new_models_bundle = existing_models.copy()
    categories = df_features['Vehicle_Category'].unique()

    updates_made = False

    for cat in categories:
        print(f"\nðŸš— Training Monthly Model for: {cat}")
        cat_df = df_features[df_features['Vehicle_Category'] == cat].copy()

        model, scaler, feature_names, new_mae = train_monthly_model(cat_df, cat)
        
        if model:
            # Check against existing
            should_update = True
            if cat in existing_models and 'mae' in existing_models[cat]:
                old_mae = existing_models[cat]['mae']
                print(f"  ðŸ” Comparison: New MAE ({new_mae:.2f}) vs Old MAE ({old_mae:.2f})")

                # Threshold: New must be strictly better or equal (to allow updates with new data)
                # If we have significantly MORE data, MAE might increase slightly but model is "better" because it knows more.
                # However, strictly following user instruction: "only if it gives better performance".
                if new_mae > old_mae:
                    print(f"  âŒ Performance degraded. Keeping old model.")
                    should_update = False
                else:
                    print(f"  âœ… Performance improved or matched. Updating model.")

            if should_update:
                new_models_bundle[cat] = {
                    'model': model,
                    'scaler': scaler,
                    'features': feature_names,
                    'daily_patterns': daily_patterns[cat],
                    'states': cat_df['State'].unique().tolist(),
                    'mae': new_mae # Store MAE for future comparison
                }
                updates_made = True

    # 6. Save Everything
    if updates_made or not model_path.exists():
        with open(model_path, 'wb') as f:
            pickle.dump(new_models_bundle, f)
        print(f"\nâœ… Saved updated hybrid models to {model_path}")
    else:
        print("\nâ¹ï¸  No performance improvements found. Existing models retained.")

def predict_daily_2026():
    """Generates daily predictions for 2026 using the hybrid model."""
    print("\nðŸ”® Generating Daily Forecasts for 2026...")

    model_path = MODELS_DIR / "advanced_model_monthly_hybrid.pkl"
    if not model_path.exists():
        print("âŒ Model not found. Run training first.")
        return None
        
    with open(model_path, 'rb') as f:
        models_data = pickle.load(f)
        
    # We need the LAST known data to generate features (Lags)
    # Load original data again
    df = load_and_clean_data()
    df_monthly = aggregate_to_monthly(df)
    df_features = create_monthly_features(df_monthly)

    all_predictions = []

    # Forecast horizon: 2026 (12 months)
    future_dates = pd.date_range(start='2026-01-01', end='2026-12-31', freq='MS')

    for cat, model_data in models_data.items():
        print(f"  -> Forecasting {cat}...")
        model = model_data['model']
        scaler = model_data['scaler']
        feature_names = model_data['features']
        patterns = model_data['daily_patterns']
        states = model_data['states']

        # Get last known data for this category
        cat_history = df_features[df_features['Vehicle_Category'] == cat].copy()

        for state in states:
            state_history = cat_history[cat_history['State'] == state].sort_values('Date')
            if state_history.empty: continue

            # Recursive Forecasting for 12 months
            # We append predictions to history to generate next month's lags
            current_history = state_history.copy()

            for date in future_dates:
                # 1. Create a row for this future date
                new_row = pd.DataFrame([{
                    'State': state,
                    'Vehicle_Category': cat,
                    'Date': date,
                    'Year': date.year,
                    'Month': date.month,
                    'EV_Sales_Quantity': 0 # Placeholder
                }])

                # Append temp row
                temp_df = pd.concat([current_history, new_row], ignore_index=True)

                # Re-calculate features (Rolling, Lags)
                # Note: Ideally we'd just update incrementally for speed, but re-calc is safer given the complex features
                temp_features = create_monthly_features(temp_df)

                # Get the specific row we just added (last one)
                row_to_predict = temp_features.iloc[[-1]].copy()

                # Encode State
                row_to_predict['State_Code'] = pd.Categorical(row_to_predict['State'], categories=pd.Categorical(states).categories).codes
                if 'State_Code' not in row_to_predict.columns:
                     # Fallback if categories didn't match perfectly (shouldn't happen with saved states)
                     row_to_predict['State_Code'] = -1

                # Prepare X
                X_pred = row_to_predict[feature_names]
                X_pred_scaled = scaler.transform(X_pred)

                # Predict Monthly Total
                pred_monthly_total = model.predict(X_pred_scaled)[0]
                pred_monthly_total = max(0, pred_monthly_total) # No negative sales

                # Update history with predicted value so next iteration uses it for lags
                current_history = pd.concat([
                    current_history,
                    pd.DataFrame([{
                        'State': state,
                        'Vehicle_Category': cat,
                        'Date': date,
                        'Year': date.year,
                        'Month': date.month,
                        'EV_Sales_Quantity': pred_monthly_total
                    }])
                ], ignore_index=True)

                # 2. Distribute to Daily
                month_num = date.month
                # Get number of days in this month
                days_in_month = pd.Period(date, freq='M').days_in_month

                month_weights = patterns.get(month_num, {})

                for day in range(1, days_in_month + 1):
                    # Get weight for this day (default to uniform if missing)
                    weight = month_weights.get(day, 1.0/days_in_month)

                    # Normalize weights for this specific month length if needed?
                    # The patterns were extracted from valid dates, so they should sum to roughly 1.
                    # But let's trust the raw weight * total.

                    daily_sale = pred_monthly_total * weight

                    all_predictions.append({
                        'Date': pd.Timestamp(year=2026, month=month_num, day=day),
                        'State': state,
                        'Vehicle_Category': cat,
                        'Predicted_Sales': int(daily_sale)
                    })

    # Save Prediction
    pred_df = pd.DataFrame(all_predictions)
    output_path = ROOT_DIR / "output" / "daily_predictions_2026.csv"
    output_path.parent.mkdir(exist_ok=True)
    pred_df.to_csv(output_path, index=False)
    print(f"âœ… Generated {len(pred_df)} daily predictions for 2026.")
    print(f"ðŸ“ Saved to: {output_path}")
    return output_path

def generate_model_performance_report():
    """
    Evaluates the saved Monthly Hybrid model and generates a performance report.
    """
    print("\nðŸ“Š Generating Model Performance Report...")

    model_path = MODELS_DIR / "advanced_model_monthly_hybrid.pkl"
    if not model_path.exists():
        print("âŒ Model not found. Run training first.")
        return None

    with open(model_path, 'rb') as f:
        models_data = pickle.load(f)

    # Load data for evaluation
    df = load_and_clean_data()
    df_monthly = aggregate_to_monthly(df)
    df_features = create_monthly_features(df_monthly)

    report_data = []

    for cat, model_data in models_data.items():
        print(f"  -> Evaluating {cat}...")
        model = model_data['model']
        scaler = model_data['scaler']
        feature_names = model_data['features']

        # Get data for this category
        cat_df = df_features[df_features['Vehicle_Category'] == cat].copy()

        if cat_df.empty: continue

        # Prepare X and y
        # Re-encode State
        cat_df['State'] = cat_df['State'].astype('category')
        # We need to ensure state codes match training. Ideally we use the same encoder.
        # But here we just use the code. If states match those in training, codes will match
        # if we sort or use the saved state list.
        # Let's map states using the saved state list to be safe.
        train_states = model_data['states']
        cat_df = cat_df[cat_df['State'].isin(train_states)] # Filter for known states
        cat_df['State_Code'] = pd.Categorical(cat_df['State'], categories=train_states).codes

        X = cat_df[feature_names]
        y = cat_df['EV_Sales_Quantity']

        # Transform
        X_scaled = scaler.transform(X)

        # Predict
        y_pred = model.predict(X_scaled)
        y_pred = np.maximum(0, y_pred)

        # Metrics
        r2 = r2_score(y, y_pred)
        mae = mean_absolute_error(y, y_pred)
        rmse = np.sqrt(np.mean((y - y_pred)**2))

        report_data.append({
            'Vehicle_Category': cat,
            'R2_Score': round(r2, 4),
            'MAE': round(mae, 2),
            'RMSE': round(rmse, 2)
        })

    report_df = pd.DataFrame(report_data)
    output_path = ROOT_DIR / "output" / "model_performance_report.csv"
    output_path.parent.mkdir(exist_ok=True)
    report_df.to_csv(output_path, index=False)

    print(f"âœ… Performance report generated.")
    print(report_df.to_string(index=False))
    print(f"ðŸ“ Saved to: {output_path}")
    return output_path

# =================================================================================================
# LEGACY COMPATIBILITY FUNCTIONS
# Restored to support live_simulation.py which depends on them.
# =================================================================================================

def create_advanced_features(df):
    """
    Create feature set for DAILY predictions (Legacy/Simulation compatibility).
    """
    df = df.copy()
    if 'Vehicle_Category' in df.columns:
        df['Vehicle_Category'] = df['Vehicle_Category'].fillna('Unknown')

    df['Date'] = pd.to_datetime(df['Date'])
    df = df.sort_values(['State', 'Vehicle_Category', 'Date'])

    # Basic temporal features
    df['year'] = df['Date'].dt.year
    df['month'] = df['Date'].dt.month
    df['day'] = df['Date'].dt.day
    df['quarter'] = df['Date'].dt.quarter
    df['day_of_week'] = df['Date'].dt.dayofweek
    df['week_of_year'] = df['Date'].dt.isocalendar().week.astype(int)
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

    # Cyclical features
    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)
    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)
    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)
    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)

    # Lag features
    for lag in [1, 7, 30]:
        df[f'lag_{lag}'] = df.groupby(['State', 'Vehicle_Category'])['EV_Sales_Quantity'].shift(lag)

    # Rolling statistics
    for window in [7, 30]:
        grouped_rolling = df.groupby(['State', 'Vehicle_Category'])['EV_Sales_Quantity'].rolling(window=window, min_periods=1)
        df[f'rolling_mean_{window}'] = grouped_rolling.mean().reset_index(level=[0, 1], drop=True)
        df[f'rolling_std_{window}'] = grouped_rolling.std().reset_index(level=[0, 1], drop=True)

    df = df.fillna(0)
    return df

def prepare_features_for_prediction(df, feature_names, scaler):
    """
    Prepares a dataframe for prediction using a pre-fitted scaler.
    """
    if 'Month_Name' in df.columns:
        df = df.drop(columns=['Month_Name'])

    feature_columns = [f for f in feature_names if f in df.columns]

    # Ensure categorical columns are present and set the type
    if 'State' in df.columns:
        df['State'] = df['State'].astype('category')
    if 'Vehicle_Category' in df.columns:
        df['Vehicle_Category'] = df['Vehicle_Category'].astype('category')

    # Encode categorical variables
    df_encoded = df.copy()
    if 'State' in df_encoded.columns:
        df_encoded['State'] = df_encoded['State'].cat.codes
    if 'Vehicle_Category' in df_encoded.columns:
        df_encoded['Vehicle_Category'] = df_encoded['Vehicle_Category'].cat.codes

    # Select the final feature set
    X = df_encoded[feature_columns]

    # Use the pre-fitted scaler to transform the data
    X_scaled = scaler.transform(X)

    return X_scaled

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == 'predict':
        predict_daily_2026()
    else:
        main_train()
